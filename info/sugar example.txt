Gradient Boosted Regression Trees (GBRT) - Simple Walkthrough Example
==================================================================

We will use a toy dataset where the input feature is the number of carbs in a meal (x)
and the target is the glucose level (y).

Step 1. Data
------------
Suppose we observe the following meals and glucose values:

| Meal carbs (x) | Glucose (y) |
|----------------|-------------|
| 10             | 120         |
| 20             | 135         |
| 30             | 150         |
| 40             | 165         |

We want to learn a model that predicts glucose given carbs.

Step 2. Initialization
-----------------------
Gradient boosting starts with a very simple model: a constant prediction.

F0(x) = mean(y)
     = (120 + 135 + 150 + 165) / 4
     = 142.5

So at the beginning, the model always predicts 142.5, regardless of x.

Step 3. Residuals
-----------------
Residuals = actual y - predicted y.

| x   | y   | F0(x) | Residual (r = y - F0(x)) |
|-----|-----|-------|---------------------------|
| 10  | 120 | 142.5 | -22.5                    |
| 20  | 135 | 142.5 | -7.5                     |
| 30  | 150 | 142.5 | +7.5                     |
| 40  | 165 | 142.5 | +22.5                    |

These residuals represent the errors the model made.

Step 4. Fit a Tree to Residuals
-------------------------------
We now train a small regression tree (a weak learner) to predict these residuals.

The tree might split at x = 25:
- If x <= 25, residual ≈ average(-22.5, -7.5) = -15
- If x > 25, residual ≈ average(7.5, 22.5) = +15

So our first tree h1(x) is:
h1(x) = -15 if x <= 25
h1(x) = +15 if x > 25

Step 5. Update the Model
-------------------------
We update the model by adding a scaled version of this tree to F0.

Let learning rate ν = 0.1.

F1(x) = F0(x) + ν * h1(x)

For x <= 25:
F1(x) = 142.5 + 0.1 * (-15) = 142.5 - 1.5 = 141.0

For x > 25:
F1(x) = 142.5 + 0.1 * (+15) = 142.5 + 1.5 = 144.0

Step 6. New Predictions
-----------------------
| x   | y   | Old pred (142.5) | New pred F1(x) | New residual (y - F1(x)) |
|-----|-----|------------------|----------------|---------------------------|
| 10  | 120 | 142.5            | 141.0          | -21.0                     |
| 20  | 135 | 142.5            | 141.0          | -6.0                      |
| 30  | 150 | 142.5            | 144.0          | +6.0                      |
| 40  | 165 | 142.5            | 144.0          | +21.0                     |

Notice how the residuals are smaller than before. The model is improving.

Step 7. Repeat
--------------
We repeat the process:
- Compute new residuals (errors left over).
- Fit another shallow tree to them.
- Add that tree (scaled by learning rate) to the model.

Each iteration refines the model, gradually reducing errors.

Intuition
---------
- The first tree shifts the flat average prediction into two regions (low vs. high carbs).
- Each additional tree keeps nudging predictions closer to the true glucose values.
- After enough trees, the model captures the underlying linear trend.

Key Points
----------
- GBRT builds an additive model, step by step.
- Each tree corrects the mistakes of the previous ensemble.
- The learning rate ensures small, careful steps.
- Shallow trees (weak learners) combine into a strong predictor.

This example shows how gradient boosting starts from a naive model (the mean) and iteratively improves predictions by focusing on residuals.
